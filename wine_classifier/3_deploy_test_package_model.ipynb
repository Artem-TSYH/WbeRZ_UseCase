{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6e590a",
   "metadata": {},
   "source": [
    "## Wine Quality Classifier (Model Deplyoment and Packaging):\n",
    "* [Read Model from MLFlow Server](#read-model-mlflow)\n",
    "* [Model Loaded Properly - Prediction on 6 DataPoints](#loaded-model-performance)\n",
    "* [Local MLFlow Model Server](#local-model-server)\n",
    "* [Model Packaging using Docker](#model-packaging)\n",
    "* [MLFlow Model Server on OpenShift](#remote-model-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093bf78c-041f-44ed-9735-e1582cbf770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf8634",
   "metadata": {},
   "source": [
    "<a id=\"read-model-mlflow\"></a>\n",
    "# Read Model from MLFlow Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ff1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Use the MLFlow instance deployed on the openshift cluster\n",
    "mlflow.set_tracking_uri(uri=\"http://mlflow-mlflow.apps.cluster-db46l.dynamic.redhatworkshops.io/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f3227a-21c2-4675-887d-e37a2548b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 12.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Model from mlflow in loaded_model\n",
    "MODEL_NAME = \"ElasticnetWineModel\"\n",
    "MODEL_VERSION= 2\n",
    "MODEL_URI = f\"models:/{MODEL_NAME}/{MODEL_VERSION}\"\n",
    "loaded_model = mlflow.pyfunc.load_model(MODEL_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c0388",
   "metadata": {},
   "source": [
    "<a id=\"loaded-model-performance\"></a>\n",
    "# Model Loaded Properly - Prediction on 6 DataPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1c1333-9434-4f1e-96f6-41dffbe57a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted Wine Quality  Actual Wine Quality\n",
      "0                       0                    0\n",
      "1                       1                    1\n",
      "2                       1                    1\n",
      "3                       0                    1\n",
      "4                       1                    1\n",
      "5                       1                    1\n"
     ]
    }
   ],
   "source": [
    "# Load testing data and assign the X_test and y_test\n",
    "test_df = pd.read_csv('testing_data/test_dataset.csv')\n",
    "\n",
    "# Test Model Performance on 6 data points from testing_data/test_dataset.csvt\n",
    "test_df_6 = test_df.head(6)\n",
    "\n",
    "# Prepare the test input by dropping the 'best quality' columns\n",
    "test_df_6_input = test_df_6.drop(['best quality'], axis=1)\n",
    "\n",
    "# Extract the actual Wine Quality for the first 6 examples from test_dataset\n",
    "actual_class_test = test_df_6['best quality']\n",
    "\n",
    "# Use the trained model to predict the class for the test input\n",
    "predicted_class_test = pd.DataFrame(loaded_model.predict(test_df_6_input), columns=['Predicted Wine Quality'])\n",
    "\n",
    "# Combine predicted and actual classes into a single DataFrame\n",
    "model_output = pd.concat([predicted_class_test, actual_class_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Rename columns for clarity\n",
    "model_output.columns = ['Predicted Wine Quality', 'Actual Wine Quality']\n",
    "\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacf087",
   "metadata": {},
   "source": [
    "<a id=\"local-model-server\"></a>\n",
    "# Local MLFlow Model Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665149d",
   "metadata": {},
   "source": [
    "### Run this commands in git bash to \n",
    "* add the MLFlow instance on the OpenShift as tracking server\n",
    "* and start a model server locally\n",
    "\n",
    "```\n",
    "export MLFLOW_TRACKING_URI=http://mlflow-mlflow.apps.cluster-db46l.dynamic.redhatworkshops.io\n",
    "```\n",
    "\n",
    "```\n",
    "mlflow models serve -m \"models:/MODEL_NAME/MODEL_VERSION\" --env-manager local --no-conda\n",
    "i.e mlflow models serve -m \"models:/ElasticnetWineModel/1\" --env-manager local --no-conda\n",
    "```\n",
    "\n",
    "when it is running, we can send prediction requests to the endpoint: *\"http://127.0.0.1:5000/invocations\"*, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d5763c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': [0, 1, 2, 3, 4, 5],\n",
       " 'columns': ['fixed acidity',\n",
       "  'volatile acidity',\n",
       "  'citric acid',\n",
       "  'residual sugar',\n",
       "  'chlorides',\n",
       "  'free sulfur dioxide',\n",
       "  'total sulfur dioxide',\n",
       "  'density',\n",
       "  'pH',\n",
       "  'sulphates',\n",
       "  'alcohol'],\n",
       " 'data': [[4.6,\n",
       "   0.445,\n",
       "   0.0,\n",
       "   1.4,\n",
       "   0.053,\n",
       "   11.0,\n",
       "   178.0,\n",
       "   0.99426,\n",
       "   3.79,\n",
       "   0.55,\n",
       "   10.2],\n",
       "  [6.5, 0.18, 0.33, 1.4, 0.029, 35.0, 138.0, 0.99114, 3.36, 0.6, 11.5],\n",
       "  [8.3, 0.16, 0.37, 7.9, 0.025, 38.0, 107.0, 0.99306, 2.93, 0.37, 11.9],\n",
       "  [5.9, 0.44, 0.36, 2.5, 0.03, 12.0, 73.0, 0.99201, 3.22, 0.48, 10.8],\n",
       "  [5.8, 0.18, 0.28, 1.3, 0.034, 9.0, 94.0, 0.99092, 3.21, 0.52, 11.2],\n",
       "  [7.3, 0.2, 0.39, 2.3, 0.048, 24.0, 87.0, 0.99044, 2.94, 0.35, 12.0]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the inference input \n",
    "test_df_6_input.to_dict(orient ='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf2f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inference Requests\n",
    "inference_request = {\n",
    "    \"dataframe_split\": \n",
    "        test_df_6_input.to_dict(orient ='split')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d4521f-033a-4bd3-ad17-f8b650bcdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted Wine Quality  Actual Wine Quality\n",
      "0                       0                    0\n",
      "1                       1                    1\n",
      "2                       1                    1\n",
      "3                       0                    1\n",
      "4                       1                    1\n",
      "5                       1                    1\n"
     ]
    }
   ],
   "source": [
    "# Send a prediction request to the depoloyed model\n",
    "\n",
    "endpoint = \"http://127.0.0.1:5000/invocations\"\n",
    "response = requests.post(endpoint, json=inference_request)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Process the prediction response\n",
    "    predictions = pd.DataFrame(response.json()['predictions'], columns=['Predicted Wine Quality'])\n",
    "\n",
    "    # Combine predictions with actual classes\n",
    "    actual_class_test = test_df_6['best quality'].reset_index(drop=True)\n",
    "    model_output = pd.concat([predictions, actual_class_test], axis=1)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    model_output.columns = ['Predicted Wine Quality', 'Actual Wine Quality']\n",
    "\n",
    "    # Display the final output\n",
    "    print(model_output)\n",
    "    \n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "    print(f\"Response content: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea5fc3",
   "metadata": {},
   "source": [
    "Now, stop the mlflow model server with ***\"crtl+c\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafef20",
   "metadata": {},
   "source": [
    "<a id=\"model-packaging\"></a>\n",
    "# Model Packaging using Docker\n",
    "\n",
    "**!!! Attention !!!** \n",
    "\n",
    "When **Dockerfile** is generated, this line should be added to be able to deploy the image onto OpenShift cluster since only **non-root Containers** are allowed to be deployed. \n",
    "\n",
    "Add this line:\n",
    "\n",
    "__*\"RUN chgrp -R 0 /opt && chmod -R g=u /opt\"*__\n",
    "\n",
    "after this line:\n",
    "\n",
    "__*\"RUN chmod o+rwX /opt/mlflow/\"*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c30fd2-a9e8-4406-a5e0-8f4aecc3eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/28 12:56:06 INFO mlflow.models.cli: Generating Dockerfile for model models:/ElasticnetWineModel/2\n",
      "\n",
      "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading artifacts:  20%|██        | 1/5 [00:00<00:00,  6.14it/s]\n",
      "Downloading artifacts:  20%|██        | 1/5 [00:00<00:00,  6.14it/s]\n",
      "Downloading artifacts:  40%|████      | 2/5 [00:00<00:00,  6.76it/s]\n",
      "Downloading artifacts:  40%|████      | 2/5 [00:00<00:00,  6.76it/s]\n",
      "Downloading artifacts:  60%|██████    | 3/5 [00:00<00:00,  6.76it/s]\n",
      "Downloading artifacts:  80%|████████  | 4/5 [00:00<00:00,  6.76it/s]\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00,  6.76it/s]\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 14.28it/s]\n",
      "2025/01/28 12:56:08 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "\n",
      "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading artifacts:  20%|██        | 1/5 [00:00<00:00,  5.53it/s]\n",
      "Downloading artifacts:  20%|██        | 1/5 [00:00<00:00,  5.53it/s]\n",
      "Downloading artifacts:  40%|████      | 2/5 [00:00<00:00,  5.53it/s]\n",
      "Downloading artifacts:  60%|██████    | 3/5 [00:00<00:00,  5.53it/s]\n",
      "Downloading artifacts:  80%|████████  | 4/5 [00:00<00:00,  5.53it/s]\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00,  5.53it/s]\n",
      "Downloading artifacts: 100%|██████████| 5/5 [00:00<00:00, 23.47it/s]\n",
      "2025/01/28 12:56:09 INFO mlflow.models.cli: Generated Dockerfile in directory wine_clf_package\n"
     ]
    }
   ],
   "source": [
    "# Generate Dockerfile and dependencies to package the model \n",
    "!mlflow models generate-dockerfile -m \"{MODEL_URI}\" --env-manager local -d wine_clf_package --enable-mlserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010657a2",
   "metadata": {},
   "source": [
    "<a id=\"remote-model-server\"></a>\n",
    "# MLFlow Model Server on OpenShift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61732f1-ba6a-4631-8d67-7a547c055e7e",
   "metadata": {},
   "source": [
    "### It is time to build and deploy the model onto the OpenShift Clsuter\n",
    "\n",
    "1. First push this new generated directory with Dockerfile and model artifacts to the preferred git (GitHub / GitLab / Azure DevOps / ...) repository. \n",
    "2. Then we go to OpenShift console to start a Build Process to package the model in an Image\n",
    "3. Deploy the Image onto the OpenShift cluster. \n",
    "4. We come back to this notebook to test the deployed model on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f7316c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predicted Wine Quality  Actual Wine Quality\n",
      "0                       0                    0\n",
      "1                       1                    0\n",
      "2                       0                    0\n",
      "3                       1                    1\n",
      "4                       1                    1\n",
      "5                       1                    1\n"
     ]
    }
   ],
   "source": [
    "# Send a prediction request to the depoloyed model on the OpenShift Cluster\n",
    "BASE_URL = \"http://test-clf-app-test.apps.cluster-db46l.dynamic.redhatworkshops.io\"\n",
    "endpoint = f\"{BASE_URL}/invocations\"\n",
    "response = requests.post(endpoint, json=inference_request)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Process the prediction response\n",
    "    predictions = pd.DataFrame(response.json()['predictions'], columns=['Predicted Wine Quality'])\n",
    "\n",
    "    # Combine predictions with actual classes\n",
    "    actual_class_test = test_df_6['best quality'].reset_index(drop=True)\n",
    "    model_output = pd.concat([predictions, actual_class_test], axis=1)\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    model_output.columns = ['Predicted Wine Quality', 'Actual Wine Quality']\n",
    "\n",
    "    # Display the final output\n",
    "    print(model_output)\n",
    "    \n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "    print(f\"Response content: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de707b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82df5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01844da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f2b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7716bca3",
   "metadata": {},
   "source": [
    "IGNORE BELOW CELLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92961bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !set MLFLOW_TRACKING_URL=\"http://mlflow-mlflow.apps.cluster-db46l.dynamic.redhatworkshops.io\"\n",
    "# !set MLFLOW_TRACKING_USERNAME=user\n",
    "# !set MLFLOW_TRACKING_PASSWORD=user\n",
    "import os\n",
    "\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow-mlflow.apps.cluster-db46l.dynamic.redhatworkshops.io\"\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:8000\"\n",
    "# os.env[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
